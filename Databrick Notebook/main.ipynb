{"cells":[{"cell_type":"markdown","source":["# Data Filtering Project"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84a549f0-df79-4b90-a64b-d22daf3c54a4"}}},{"cell_type":"code","source":["fileName = dbutils.widgets.get('fileName')\nfileNameWithoutExt = fileName.split('.')[0]\nprint(fileNameWithoutExt)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Widget function to get the file name from the adf","showTitle":true,"inputWidgets":{},"nuid":"58fbf49e-f6a3-4be2-a21f-087946ac0595"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["import pyspark.sql.functions as F\n#from datetime import datetme as dt\n\n#Just change all the values here based on the resource name you have created in your environemnt and workspace.\n\nsqlDbName = 'apmorgan'\ndbUserName = 'azureify'\npasswordKey = 'sqlDbPasswordKey'\nstgAccountSASTokenKey = 'sastokenforstg'\nlandingFileName =fileName #'Product'  #dbutils.widgets.get('Product')\ndatabricksScopeName ='dataFilteringProjectScope'\ndbServer = 'apmorgantest'\ndbServerPortNumber = '1433'\nstorageContainer = 'inputdata'\nstorageAccount = 'casynapse256'\nlandingMountPoint = '/mnt'\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Defining keys for sql server and adls gen2","showTitle":true,"inputWidgets":{},"nuid":"6259a115-ab8d-46f2-9a36-7bda72962b56"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">wasbs://demo@azureblobstg.blob,core.windows.net\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">wasbs://demo@azureblobstg.blob,core.windows.net\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["if not any(mount.mountPoint == landingMountPoint for mount in dbutils.fs.mounts()):\n    dbutils.fs.mount( source = f'wasbs://{storageContainer}@{storageAccount}.blob.core.windows.net', mount_point= landingMountPoint, extra_configs ={f'fs.azure.sas.{storageContainer}.{storageAccount}.blob.core.windows.net':dbutils.secrets.get(scope = databricksScopeName, key= stgAccountSASTokenKey)})\n    print('Mounted the storage account successfully')\nelse:\n    print('Storage account already mounted')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Mounting the Adls Gen2","showTitle":true,"inputWidgets":{},"nuid":"1791d71e-246d-4fb9-ab59-9283fc2a0267"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Mounted the storage account successfully\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Mounted the storage account successfully\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#connect to Azure SQL DB\ndbPassword = dbutils.secrets.get(scope = databricksScopeName, key= passwordKey)\nserverurl = f'jdbc:sqlserver://{dbServer}.database.windows.net:{dbServerPortNumber};database={sqlDbName};user={dbUserName};'\nconnectionProperties = {\n    'password':dbPassword,\n    'driver':'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n}\ndf = spark.read.jdbc(url = serverurl, table = 'dbo.FileDetailsFormat', properties= connectionProperties)\ndisplay(df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Connecting to Sql Servert","showTitle":true,"inputWidgets":{},"nuid":"be56aba3-aee9-4567-8590-49cbf7dc6eb9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1,"Product","StartDate","yyyy-dd-MM","true","2012-06-18T22:34:09.000+0000"],[1,"Product","EndDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[1,"Product","CreateDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[1,"Product","ModifiedDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[2,"ProductDescription","ModifiedDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[2,"ProductDescription","StartDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[2,"ProductDescription","EndDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[3,"CustomerDetail","CreateDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"],[3,"CustomerDetail","ActiveDate","yyyy/dd/MM","true","2012-06-18T22:34:09.000+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":[],"pivotAggregation":null,"xColumns":[],"yColumns":[]},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"FileNo","type":"\"integer\"","metadata":"{}"},{"name":"FileName","type":"\"string\"","metadata":"{}"},{"name":"ColumnName","type":"\"string\"","metadata":"{}"},{"name":"ColumnDateFormat","type":"\"string\"","metadata":"{}"},{"name":"ColumnIsNull","type":"\"string\"","metadata":"{}"},{"name":"ModifiedDate","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FileNo</th><th>FileName</th><th>ColumnName</th><th>ColumnDateFormat</th><th>ColumnIsNull</th><th>ModifiedDate</th></tr></thead><tbody><tr><td>1</td><td>Product</td><td>StartDate</td><td>yyyy-dd-MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>1</td><td>Product</td><td>EndDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>1</td><td>Product</td><td>CreateDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>1</td><td>Product</td><td>ModifiedDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>2</td><td>ProductDescription</td><td>ModifiedDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>2</td><td>ProductDescription</td><td>StartDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>2</td><td>ProductDescription</td><td>EndDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>3</td><td>CustomerDetail</td><td>CreateDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr><tr><td>3</td><td>CustomerDetail</td><td>ActiveDate</td><td>yyyy/dd/MM</td><td>true</td><td>2012-06-18T22:34:09.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv('/mnt/landing/'+fileName, inferSchema=True, header=True)\n\nerrorFlag=False\nerrorMessage = ''\ntotalcount = df1.count()\nprint(totalcount)\ndistinctCount = df1.distinct().count()\nprint(distinctCount)\nif distinctCount !=totalcount:\n    errorFlag = True\n    errorMessage = 'Duplication Found. Rule 1 Failed'\nprint(errorMessage)\n    \n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Checking for the duplications","showTitle":true,"inputWidgets":{},"nuid":"638c358e-b149-4675-ad1c-33376b3d542e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"exit","data":"{\"errorFlag\": \"true\", \"errorMessage\":\"Duplication Found. Rule 1 Failed DateFormate is incorrect for StartDate  DateFormate is incorrect for EndDate  DateFormate is incorrect for CreateDate  DateFormate is incorrect for ModifiedDate \"}","arguments":{},"metadata":{}}},"output_type":"display_data","data":{"text/plain":["{\"errorFlag\": \"true\", \"errorMessage\":\"Duplication Found. Rule 1 Failed DateFormate is incorrect for StartDate  DateFormate is incorrect for EndDate  DateFormate is incorrect for CreateDate  DateFormate is incorrect for ModifiedDate \"}"]}}],"execution_count":0},{"cell_type":"code","source":["df2 = df.filter(df.FileName==fileNameWithoutExt).select('ColumnName','ColumnDateFormat' )\nrows = df2.collect()\nfor r in rows:\n    colName = r[0]\n    colFormat =r[1]\n    print(colName, colFormat)\n    formatCount =df1.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n    if formatCount == totalcount:\n        errorFlag = True\n        errorMessage = errorMessage +' DateFormate is incorrect for {} '.format(colName)\n    else:\n        print('All rows are good for ', colName)\npritnt(errorMessage)\n\nif errorFlag:\n    dbutils.fs.mv(f'/mnt/landing/{fileName}',f'/mnt/rejected/{fileName}')\n    dbutils.notebook.exit('{\"errorFlag\": \"true\", \"errorMessage\":\"'+errorMessage +'\"}')\nelse:\n    dbutils.fs.mv(f'/mnt/landing/{fileName}',f'/mnt/staging/{fileName}')\n    dbutils.notebook.exit('{\"errorFlag\": \"false\", \"errorMessage\":\"No error\"}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Checking the schema of the files ","showTitle":true,"inputWidgets":{},"nuid":"c67847d7-ba43-436b-83b1-ff775db1c2c0"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Project2 (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4321443721589747}},"nbformat":4,"nbformat_minor":0}
